{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pprint\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_import(name):\n",
    "    import sys, os, csv\n",
    "    from pathlib import Path\n",
    "    from toolz import concatv\n",
    "    cwd = Path()\n",
    "    reader = sorted(concatv(\n",
    "        [('sys', 'script', sys.argv[0], name)],\n",
    "        [('sys', 'executable', sys.executable, name)],\n",
    "        [('path', 'cwd', str(cwd.absolute()), name)],\n",
    "        sorted(('module', k, str(mod), name) for k, mod in sys.modules.items()),\n",
    "        sorted(('environ', k, str(val), name) for k, val in dict(os.environ).items()),\n",
    "    ))\n",
    "    \n",
    "    infile = cwd /f\"{name}.csv\"\n",
    "\n",
    "    with infile.open('w') as fh:\n",
    "        writer = csv.writer(fh)\n",
    "        writer.writerow(['category','type','item', 'source'])\n",
    "        for rec in reader:\n",
    "            writer.writerow(rec)\n",
    "    result = {'infile': str(infile),\n",
    "            'record_cnt': len(reader)}\n",
    "    print(f'>> result={result}')\n",
    "    import _scproxy\n",
    "    #import numpy\n",
    "    #import pandas\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> result={'infile': 'notebook_local.csv', 'record_cnt': 972}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'infile': 'notebook_local.csv', 'record_cnt': 972}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = try_import('notebook_local')\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:490)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:479)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:597)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:490)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:479)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:597)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c898f91cc3e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtry_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'notebook_spark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:490)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:479)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:597)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:490)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:479)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:597)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Import Test\")\n",
    "        .getOrCreate() )\n",
    "df = spark.createDataFrame([{'iteration': 1} ])\n",
    "rdd = df.rdd.map(lambda x: try_import('notebook_spark'))\n",
    "ret = rdd.take(1)\n",
    "pprint.pprint(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Path().glob('cmd*.csv')\n",
    "reader = map(pd.read_csv,reader)\n",
    "df = pd.concat(reader)\n",
    "df = df.set_index(['category','type','source']).item.unstack()\n",
    "df= df.sort_index()\n",
    "df['match'] = df.fillna('na').cmd_local == df.fillna('na').cmd_spark\n",
    "df = df[~df.match].copy()\n",
    "DATA = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= DATA.xs('module').copy()\n",
    "ds = df.cmd_local.str.extract(r\"from '(.+)'\")\n",
    "df['cmd_local'] = ds.where(ds.notna(), df.cmd_local, axis=0)\n",
    "\n",
    "ds = df.cmd_spark.str.extract(r\"from '(.+)'\")\n",
    "df['cmd_spark'] = ds.where(ds.notna(), df.cmd_spark, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>source</th>\n",
       "      <th>cmd_local</th>\n",
       "      <th>cmd_spark</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>__main__</th>\n",
       "      <td>./import_test.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;module 'org' (namespace)&gt;</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/__init__.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/__init__.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.compat</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/compat.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/compat.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.finalizer</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/finalizer.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/finalizer.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.java_collections</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/java_collections.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.java_gateway</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/java_gateway.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.protocol</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/protocol.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.signals</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/signals.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/signals.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>py4j.version</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/version.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/version.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/__init__.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark._globals</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/_globals.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/_globals.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.accumulators</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/accumulators.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/accumulators.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.broadcast</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/broadcast.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/broadcast.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.cloudpickle</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/cloudpickle.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.conf</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/conf.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/conf.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.context</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/context.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/context.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.files</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/files.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/files.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.find_spark_home</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/find_spark_home.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/find_spark_home.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.heapq3</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/heapq3.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/heapq3.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.java_gateway</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/java_gateway.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/java_gateway.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.join</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/join.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/join.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.profiler</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/profiler.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/profiler.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.rdd</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/rdd.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.rddsampler</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/rddsampler.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/rddsampler.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.resultiterable</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/resultiterable.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/resultiterable.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.serializers</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/serializers.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.shuffle</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/shuffle.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/__init__.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/__init__.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.catalog</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/catalog.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/catalog.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.column</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/column.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.conf</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/conf.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/conf.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.context</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/context.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/context.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.dataframe</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/dataframe.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.group</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/group.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/group.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.readwriter</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/readwriter.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.session</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/session.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.streaming</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/streaming.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.types</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/types.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.udf</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/udf.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/udf.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.utils</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/utils.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.sql.window</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/window.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/window.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.statcounter</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/statcounter.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.status</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/status.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/status.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.storagelevel</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/storagelevel.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/storagelevel.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.taskcontext</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/taskcontext.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/taskcontext.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.traceback_utils</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/traceback_utils.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/traceback_utils.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.util</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/util.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/util.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.version</th>\n",
       "      <td>/Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/version.py</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/version.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyspark.worker</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/worker.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/wmcabee/.pyenv/versions/3.7.4/lib/python3.7/lib-dynload/resource.cpython-37m-darwin.so</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>runpy</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/wmcabee/.pyenv/versions/3.7.4/lib/python3.7/runpy.py</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "source                                                                                                      cmd_local  \\\n",
       "type                                                                                                                    \n",
       "__main__                                                                                             ./import_test.py   \n",
       "org                                                                                                               NaN   \n",
       "py4j                               /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/__init__.py   \n",
       "py4j.compat                          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/compat.py   \n",
       "py4j.finalizer                    /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/finalizer.py   \n",
       "py4j.java_collections      /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/java_collections.py   \n",
       "py4j.java_gateway              /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/java_gateway.py   \n",
       "py4j.protocol                      /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/protocol.py   \n",
       "py4j.signals                        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/signals.py   \n",
       "py4j.version                        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/py4j/version.py   \n",
       "pyspark                         /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/__init__.py   \n",
       "pyspark._globals                /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/_globals.py   \n",
       "pyspark.accumulators        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/accumulators.py   \n",
       "pyspark.broadcast              /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/broadcast.py   \n",
       "pyspark.cloudpickle          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/cloudpickle.py   \n",
       "pyspark.conf                        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/conf.py   \n",
       "pyspark.context                  /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/context.py   \n",
       "pyspark.files                      /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/files.py   \n",
       "pyspark.find_spark_home  /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/find_spark_home.py   \n",
       "pyspark.heapq3                    /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/heapq3.py   \n",
       "pyspark.java_gateway        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/java_gateway.py   \n",
       "pyspark.join                        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/join.py   \n",
       "pyspark.profiler                /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/profiler.py   \n",
       "pyspark.rdd                          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/rdd.py   \n",
       "pyspark.rddsampler            /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/rddsampler.py   \n",
       "pyspark.resultiterable    /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/resultiterable.py   \n",
       "pyspark.serializers          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/serializers.py   \n",
       "pyspark.shuffle                  /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/shuffle.py   \n",
       "pyspark.sql                 /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/__init__.py   \n",
       "pyspark.sql.catalog          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/catalog.py   \n",
       "pyspark.sql.column            /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/column.py   \n",
       "pyspark.sql.conf                /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/conf.py   \n",
       "pyspark.sql.context          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/context.py   \n",
       "pyspark.sql.dataframe      /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/dataframe.py   \n",
       "pyspark.sql.group              /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/group.py   \n",
       "pyspark.sql.readwriter    /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/readwriter.py   \n",
       "pyspark.sql.session          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/session.py   \n",
       "pyspark.sql.streaming      /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/streaming.py   \n",
       "pyspark.sql.types              /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/types.py   \n",
       "pyspark.sql.udf                  /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/udf.py   \n",
       "pyspark.sql.utils              /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/utils.py   \n",
       "pyspark.sql.window            /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/sql/window.py   \n",
       "pyspark.statcounter          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/statcounter.py   \n",
       "pyspark.status                    /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/status.py   \n",
       "pyspark.storagelevel        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/storagelevel.py   \n",
       "pyspark.taskcontext          /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/taskcontext.py   \n",
       "pyspark.traceback_utils  /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/traceback_utils.py   \n",
       "pyspark.util                        /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/util.py   \n",
       "pyspark.version                  /Users/wmcabee/_NBC/nbc_analysis/venv/lib/python3.7/site-packages/pyspark/version.py   \n",
       "pyspark.worker                                                                                                    NaN   \n",
       "resource                                                                                                          NaN   \n",
       "runpy                                                                                                             NaN   \n",
       "\n",
       "source                                                                                                       cmd_spark  \\\n",
       "type                                                                                                                     \n",
       "__main__                                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/daemon.py   \n",
       "org                                                                                         <module 'org' (namespace)>   \n",
       "py4j                                          /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/__init__.py   \n",
       "py4j.compat                                     /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/compat.py   \n",
       "py4j.finalizer                               /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/finalizer.py   \n",
       "py4j.java_collections                 /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py   \n",
       "py4j.java_gateway                         /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py   \n",
       "py4j.protocol                                 /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py   \n",
       "py4j.signals                                   /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/signals.py   \n",
       "py4j.version                                   /Users/wmcabee/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/version.py   \n",
       "pyspark                                            /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py   \n",
       "pyspark._globals                                   /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/_globals.py   \n",
       "pyspark.accumulators                           /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/accumulators.py   \n",
       "pyspark.broadcast                                 /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/broadcast.py   \n",
       "pyspark.cloudpickle                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py   \n",
       "pyspark.conf                                           /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/conf.py   \n",
       "pyspark.context                                     /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/context.py   \n",
       "pyspark.files                                         /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/files.py   \n",
       "pyspark.find_spark_home                     /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/find_spark_home.py   \n",
       "pyspark.heapq3                                       /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/heapq3.py   \n",
       "pyspark.java_gateway                           /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/java_gateway.py   \n",
       "pyspark.join                                           /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/join.py   \n",
       "pyspark.profiler                                   /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/profiler.py   \n",
       "pyspark.rdd                                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py   \n",
       "pyspark.rddsampler                               /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/rddsampler.py   \n",
       "pyspark.resultiterable                       /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/resultiterable.py   \n",
       "pyspark.serializers                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py   \n",
       "pyspark.shuffle                                     /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py   \n",
       "pyspark.sql                                    /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/__init__.py   \n",
       "pyspark.sql.catalog                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/catalog.py   \n",
       "pyspark.sql.column                               /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py   \n",
       "pyspark.sql.conf                                   /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/conf.py   \n",
       "pyspark.sql.context                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/context.py   \n",
       "pyspark.sql.dataframe                         /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py   \n",
       "pyspark.sql.group                                 /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/group.py   \n",
       "pyspark.sql.readwriter                       /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py   \n",
       "pyspark.sql.session                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py   \n",
       "pyspark.sql.streaming                         /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py   \n",
       "pyspark.sql.types                                 /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py   \n",
       "pyspark.sql.udf                                     /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/udf.py   \n",
       "pyspark.sql.utils                                 /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py   \n",
       "pyspark.sql.window                               /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/sql/window.py   \n",
       "pyspark.statcounter                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py   \n",
       "pyspark.status                                       /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/status.py   \n",
       "pyspark.storagelevel                           /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/storagelevel.py   \n",
       "pyspark.taskcontext                             /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/taskcontext.py   \n",
       "pyspark.traceback_utils                     /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/traceback_utils.py   \n",
       "pyspark.util                                           /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/util.py   \n",
       "pyspark.version                                     /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/version.py   \n",
       "pyspark.worker                                       /Users/wmcabee/opt/spark/python/lib/pyspark.zip/pyspark/worker.py   \n",
       "resource                 /Users/wmcabee/.pyenv/versions/3.7.4/lib/python3.7/lib-dynload/resource.cpython-37m-darwin.so   \n",
       "runpy                                                      /Users/wmcabee/.pyenv/versions/3.7.4/lib/python3.7/runpy.py   \n",
       "\n",
       "source                   match  \n",
       "type                            \n",
       "__main__                 False  \n",
       "org                      False  \n",
       "py4j                     False  \n",
       "py4j.compat              False  \n",
       "py4j.finalizer           False  \n",
       "py4j.java_collections    False  \n",
       "py4j.java_gateway        False  \n",
       "py4j.protocol            False  \n",
       "py4j.signals             False  \n",
       "py4j.version             False  \n",
       "pyspark                  False  \n",
       "pyspark._globals         False  \n",
       "pyspark.accumulators     False  \n",
       "pyspark.broadcast        False  \n",
       "pyspark.cloudpickle      False  \n",
       "pyspark.conf             False  \n",
       "pyspark.context          False  \n",
       "pyspark.files            False  \n",
       "pyspark.find_spark_home  False  \n",
       "pyspark.heapq3           False  \n",
       "pyspark.java_gateway     False  \n",
       "pyspark.join             False  \n",
       "pyspark.profiler         False  \n",
       "pyspark.rdd              False  \n",
       "pyspark.rddsampler       False  \n",
       "pyspark.resultiterable   False  \n",
       "pyspark.serializers      False  \n",
       "pyspark.shuffle          False  \n",
       "pyspark.sql              False  \n",
       "pyspark.sql.catalog      False  \n",
       "pyspark.sql.column       False  \n",
       "pyspark.sql.conf         False  \n",
       "pyspark.sql.context      False  \n",
       "pyspark.sql.dataframe    False  \n",
       "pyspark.sql.group        False  \n",
       "pyspark.sql.readwriter   False  \n",
       "pyspark.sql.session      False  \n",
       "pyspark.sql.streaming    False  \n",
       "pyspark.sql.types        False  \n",
       "pyspark.sql.udf          False  \n",
       "pyspark.sql.utils        False  \n",
       "pyspark.sql.window       False  \n",
       "pyspark.statcounter      False  \n",
       "pyspark.status           False  \n",
       "pyspark.storagelevel     False  \n",
       "pyspark.taskcontext      False  \n",
       "pyspark.traceback_utils  False  \n",
       "pyspark.util             False  \n",
       "pyspark.version          False  \n",
       "pyspark.worker           False  \n",
       "resource                 False  \n",
       "runpy                    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('max_rows',None, 'max_columns', None, 'max_colwidth', 100):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
